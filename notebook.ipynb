{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-03T17:24:12.825421Z",
     "iopub.status.busy": "2025-01-03T17:24:12.825099Z",
     "iopub.status.idle": "2025-01-03T17:24:12.843332Z",
     "shell.execute_reply": "2025-01-03T17:24:12.842047Z",
     "shell.execute_reply.started": "2025-01-03T17:24:12.825395Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/uber-pickups-in-nyc/uber-raw-data-may14.csv\n",
      "/kaggle/input/uber-pickups-in-nyc/other-FHV-services_jan-aug-2015.csv\n",
      "/kaggle/input/uber-pickups-in-nyc/other-Prestige_B01338.csv\n",
      "/kaggle/input/uber-pickups-in-nyc/other-Firstclass_B01536.csv\n",
      "/kaggle/input/uber-pickups-in-nyc/other-Skyline_B00111.csv\n",
      "/kaggle/input/uber-pickups-in-nyc/other-Lyft_B02510.csv\n",
      "/kaggle/input/uber-pickups-in-nyc/uber-raw-data-apr14.csv\n",
      "/kaggle/input/uber-pickups-in-nyc/uber-raw-data-jul14.csv\n",
      "/kaggle/input/uber-pickups-in-nyc/other-Dial7_B00887.csv\n",
      "/kaggle/input/uber-pickups-in-nyc/other-Diplo_B01196.csv\n",
      "/kaggle/input/uber-pickups-in-nyc/other-Federal_02216.csv\n",
      "/kaggle/input/uber-pickups-in-nyc/Uber-Jan-Feb-FOIL.csv\n",
      "/kaggle/input/uber-pickups-in-nyc/uber-raw-data-jun14.csv\n",
      "/kaggle/input/uber-pickups-in-nyc/other-Highclass_B01717.csv\n",
      "/kaggle/input/uber-pickups-in-nyc/other-American_B01362.csv\n",
      "/kaggle/input/uber-pickups-in-nyc/uber-raw-data-sep14.csv\n",
      "/kaggle/input/uber-pickups-in-nyc/uber-raw-data-aug14.csv\n",
      "/kaggle/input/uber-pickups-in-nyc/other-Carmel_B00256.csv\n",
      "/kaggle/input/uber-pickups-in-nyc/uber-raw-data-janjune-15.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T17:24:16.586022Z",
     "iopub.status.busy": "2025-01-03T17:24:16.585666Z",
     "iopub.status.idle": "2025-01-03T17:24:16.606073Z",
     "shell.execute_reply": "2025-01-03T17:24:16.605000Z",
     "shell.execute_reply.started": "2025-01-03T17:24:16.585996Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/uber-pickups-in-nyc/other-Federal_02216.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T17:24:19.099405Z",
     "iopub.status.busy": "2025-01-03T17:24:19.099088Z",
     "iopub.status.idle": "2025-01-03T17:24:19.118817Z",
     "shell.execute_reply": "2025-01-03T17:24:19.116982Z",
     "shell.execute_reply.started": "2025-01-03T17:24:19.099382Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Mahnum Zahid/AppData/Local/Microsoft/WindowsApps/python3.11.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Summary Statistics (Mean, Median, Mode, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T17:24:22.696178Z",
     "iopub.status.busy": "2025-01-03T17:24:22.695782Z",
     "iopub.status.idle": "2025-01-03T17:24:22.734840Z",
     "shell.execute_reply": "2025-01-03T17:24:22.733229Z",
     "shell.execute_reply.started": "2025-01-03T17:24:22.696148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Summary of numerical columns\n",
    "print(\"Summary Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Median for numerical columns (if any numerical data exists)\n",
    "print(\"\\nMedian of numerical features:\")\n",
    "print(df.median(numeric_only=True))\n",
    "\n",
    "# Mode for categorical features\n",
    "print(\"\\nMode of all features:\")\n",
    "print(df.mode().iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Visualizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a. Countplot for Pickup and Drop-Off Addresses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T17:30:10.071089Z",
     "iopub.status.busy": "2025-01-03T17:30:10.070669Z",
     "iopub.status.idle": "2025-01-03T17:30:11.109205Z",
     "shell.execute_reply": "2025-01-03T17:30:11.107816Z",
     "shell.execute_reply.started": "2025-01-03T17:30:10.071057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the figure size and style\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Filter top 10 Pickup Addresses based on frequency\n",
    "top_n_pickup = df['PU_Address'].value_counts().head(10).index\n",
    "df_top_pickup = df[df['PU_Address'].isin(top_n_pickup)]\n",
    "\n",
    "# Countplot for top Pickup Addresses\n",
    "plt.figure(figsize=(12, 8))  # Adjusting figure size\n",
    "sns.countplot(y=df_top_pickup['PU_Address'], order=df_top_pickup['PU_Address'].value_counts().index, palette=\"viridis\")\n",
    "plt.xlabel('Count', fontsize=12)\n",
    "plt.ylabel('Pickup Address', fontsize=12)\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Filter top 10 Drop-Off Addresses based on frequency\n",
    "top_n_dropoff = df['DO_Address'].value_counts().head(10).index\n",
    "df_top_dropoff = df[df['DO_Address'].isin(top_n_dropoff)]\n",
    "\n",
    "# Countplot for top Drop-Off Addresses\n",
    "plt.figure(figsize=(12, 8))  # Adjusting figure size\n",
    "sns.countplot(y=df_top_dropoff['DO_Address'], order=df_top_dropoff['DO_Address'].value_counts().index, palette=\"coolwarm\")\n",
    "plt.xlabel('Count', fontsize=12)\n",
    "plt.ylabel('Drop-Off Address', fontsize=12)\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. Distribution of Status\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T16:45:33.938567Z",
     "iopub.status.busy": "2025-01-02T16:45:33.938140Z",
     "iopub.status.idle": "2025-01-02T16:45:34.171066Z",
     "shell.execute_reply": "2025-01-02T16:45:34.169990Z",
     "shell.execute_reply.started": "2025-01-02T16:45:33.938539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=df['Status'])\n",
    "plt.title('Distribution of Status')\n",
    "plt.xlabel('Status')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c. Trend Analysis: Number of Trips Over Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T16:45:34.172203Z",
     "iopub.status.busy": "2025-01-02T16:45:34.171900Z",
     "iopub.status.idle": "2025-01-02T16:45:34.491513Z",
     "shell.execute_reply": "2025-01-02T16:45:34.490122Z",
     "shell.execute_reply.started": "2025-01-02T16:45:34.172178Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert Date to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Group by Date and count the number of trips\n",
    "trips_per_day = df.groupby('Date').size()\n",
    "\n",
    "# Plot trips over time\n",
    "trips_per_day.plot(kind='line', figsize=(10, 6))\n",
    "plt.title('Trips Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Trips')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Correlation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T16:45:34.493387Z",
     "iopub.status.busy": "2025-01-02T16:45:34.492918Z",
     "iopub.status.idle": "2025-01-02T16:45:34.498135Z",
     "shell.execute_reply": "2025-01-02T16:45:34.496835Z",
     "shell.execute_reply.started": "2025-01-02T16:45:34.493339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Correlation matrix (if any numerical columns exist)\n",
    "# correlation_matrix = df.corr()\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "# plt.title('Correlation Heatmap')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Missing Value Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T16:45:34.502571Z",
     "iopub.status.busy": "2025-01-02T16:45:34.502258Z",
     "iopub.status.idle": "2025-01-02T16:45:34.521162Z",
     "shell.execute_reply": "2025-01-02T16:45:34.519707Z",
     "shell.execute_reply.started": "2025-01-02T16:45:34.502545Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)\n",
    "\n",
    "# Percentage of missing values\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "print(\"\\nMissing Percentage:\\n\", missing_percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Outlier Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T16:45:34.523576Z",
     "iopub.status.busy": "2025-01-02T16:45:34.523261Z",
     "iopub.status.idle": "2025-01-02T16:45:34.540970Z",
     "shell.execute_reply": "2025-01-02T16:45:34.539723Z",
     "shell.execute_reply.started": "2025-01-02T16:45:34.523547Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Example for checking outliers in a numerical column\n",
    "# sns.boxplot(x=df['numerical_column']) # Replace with your numerical column\n",
    "# plt.title('Outliers in Numerical Column')\n",
    "# plt.show()\n",
    "\n",
    "# You can also check for extreme values in categorical columns if relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Feature Distribution Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T17:44:02.517242Z",
     "iopub.status.busy": "2025-01-03T17:44:02.516905Z",
     "iopub.status.idle": "2025-01-03T17:44:03.390392Z",
     "shell.execute_reply": "2025-01-03T17:44:03.389097Z",
     "shell.execute_reply.started": "2025-01-03T17:44:02.517216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get the top 10 Pickup Addresses by frequency\n",
    "top_10_pickup_addresses = df['PU_Address'].value_counts().nlargest(10).index\n",
    "top_10_pickup_df = df[df['PU_Address'].isin(top_10_pickup_addresses)]\n",
    "\n",
    "# Set the figure size for better clarity\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Distribution of top 10 Pickup Addresses\n",
    "sns.countplot(x='PU_Address', data=top_10_pickup_df, palette='Blues_d', order=top_10_pickup_addresses)\n",
    "plt.title('Top 10 Pickup Addresses', fontsize=16)\n",
    "plt.xticks(rotation=90, fontsize=12)  # Rotate the labels and adjust font size for clarity\n",
    "plt.xlabel('Pickup Address', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.subplots_adjust(bottom=0.2)  # Increase the bottom margin to fit x-axis labels\n",
    "plt.show()\n",
    "\n",
    "# Get the top 10 Drop-Off Addresses by frequency\n",
    "top_10_dropoff_addresses = df['DO_Address'].value_counts().nlargest(10).index\n",
    "top_10_dropoff_df = df[df['DO_Address'].isin(top_10_dropoff_addresses)]\n",
    "\n",
    "# Set the figure size for better clarity\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Distribution of top 10 Drop-Off Addresses\n",
    "sns.countplot(x='DO_Address', data=top_10_dropoff_df, palette='Oranges_d', order=top_10_dropoff_addresses)\n",
    "plt.title('Top 10 Drop-Off Addresses', fontsize=16)\n",
    "plt.xticks(rotation=90, fontsize=12)  # Rotate the labels and adjust font size for clarity\n",
    "plt.xlabel('Drop-Off Address', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.subplots_adjust(bottom=0.2)  # Increase the bottom margin to fit x-axis labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Data Types and Unique Value Counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T16:45:38.188310Z",
     "iopub.status.busy": "2025-01-02T16:45:38.187980Z",
     "iopub.status.idle": "2025-01-02T16:45:38.198200Z",
     "shell.execute_reply": "2025-01-02T16:45:38.197108Z",
     "shell.execute_reply.started": "2025-01-02T16:45:38.188283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data types of columns\n",
    "print(\"Data Types of Columns:\\n\", df.dtypes)\n",
    "\n",
    "# Unique values in each column\n",
    "unique_values = df.nunique()\n",
    "print(\"\\nUnique Values in Each Column:\\n\", unique_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Trend Analysis (if applicable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T16:45:38.199823Z",
     "iopub.status.busy": "2025-01-02T16:45:38.199426Z",
     "iopub.status.idle": "2025-01-02T16:45:38.520390Z",
     "shell.execute_reply": "2025-01-02T16:45:38.519271Z",
     "shell.execute_reply.started": "2025-01-02T16:45:38.199780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert Time to datetime, allowing pandas to infer the format\n",
    "df['Time'] = pd.to_datetime(df['Time'], errors='coerce')  # 'coerce' will turn invalid parsing into NaT\n",
    "\n",
    "# Extract hour from the Time column\n",
    "df['hour'] = df['Time'].dt.hour\n",
    "\n",
    "# Trips by hour\n",
    "trips_per_hour = df['hour'].value_counts().sort_index()\n",
    "\n",
    "# Plot trips over time (by hour)\n",
    "trips_per_hour.plot(kind='line', figsize=(10, 6))\n",
    "plt.title('Trips Over Hours of the Day')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Number of Trips')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Grouped Aggregations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T16:45:38.521790Z",
     "iopub.status.busy": "2025-01-02T16:45:38.521505Z",
     "iopub.status.idle": "2025-01-02T16:45:38.532736Z",
     "shell.execute_reply": "2025-01-02T16:45:38.531583Z",
     "shell.execute_reply.started": "2025-01-02T16:45:38.521765Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Count trips by PU_Address and Status\n",
    "grouped_data = df.groupby(['PU_Address', 'Status']).size()\n",
    "print(\"Trips by Pickup Address and Status:\\n\", grouped_data)\n",
    "\n",
    "# Average trips by PU_Address\n",
    "avg_trips_by_address = df.groupby('PU_Address').size().mean()\n",
    "print(f\"Average trips by Pickup Address: {avg_trips_by_address}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Pairwise Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T17:42:56.535007Z",
     "iopub.status.busy": "2025-01-03T17:42:56.534600Z",
     "iopub.status.idle": "2025-01-03T17:42:57.351913Z",
     "shell.execute_reply": "2025-01-03T17:42:57.350539Z",
     "shell.execute_reply.started": "2025-01-03T17:42:56.534973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get the top 20 Pickup Addresses based on frequency\n",
    "top_20_pickup_addresses = df['PU_Address'].value_counts().head(20).index\n",
    "\n",
    "# Filter the dataframe to include only top 20 Pickup Addresses\n",
    "df_top_20 = df[df['PU_Address'].isin(top_20_pickup_addresses)]\n",
    "\n",
    "# Set the figure size for better clarity\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Countplot for Pickup Address vs Status (Top 20)\n",
    "sns.countplot(x='PU_Address', hue='Status', data=df_top_20, palette='Set2')\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Top 20 Pickup Addresses vs. Status', fontsize=16)\n",
    "plt.xlabel('Pickup Address', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "\n",
    "# Rotate x-axis labels for better visibility\n",
    "plt.xticks(rotation=90, fontsize=12)\n",
    "\n",
    "# Adjust the layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Geospatial Analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T16:45:42.148132Z",
     "iopub.status.busy": "2025-01-02T16:45:42.147769Z",
     "iopub.status.idle": "2025-01-02T16:45:42.670809Z",
     "shell.execute_reply": "2025-01-02T16:45:42.669900Z",
     "shell.execute_reply.started": "2025-01-02T16:45:42.148098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Check if latitude and longitude columns exist\n",
    "if 'pickup_latitude' in df.columns and 'pickup_longitude' in df.columns:\n",
    "    # Example: Center of New York City for visualization\n",
    "    map_center = [40.7128, -74.0060]  # Coordinates of NYC\n",
    "    map_uber = folium.Map(location=map_center, zoom_start=12)\n",
    "\n",
    "    # Sample 100 data points (adjust the sample size if needed)\n",
    "    for idx, row in df.sample(100).iterrows():\n",
    "        # Ensure latitudes/longitudes exist in the data row\n",
    "        if pd.notna(row['pickup_latitude']) and pd.notna(row['pickup_longitude']):\n",
    "            folium.CircleMarker(\n",
    "                location=[row['pickup_latitude'], row['pickup_longitude']],\n",
    "                radius=3,  # Adjust the radius of the circle marker\n",
    "                color='blue',  # Adjust color as needed\n",
    "                fill=True,\n",
    "                fill_color='blue',\n",
    "                fill_opacity=0.6\n",
    "            ).add_to(map_uber)\n",
    "\n",
    "    # Show the map\n",
    "    map_uber\n",
    "else:\n",
    "    print(\"Latitude and longitude data not available in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Handle Missing Values (Imputation or Removal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T16:52:38.288957Z",
     "iopub.status.busy": "2025-01-02T16:52:38.288610Z",
     "iopub.status.idle": "2025-01-02T16:52:38.311633Z",
     "shell.execute_reply": "2025-01-02T16:52:38.310520Z",
     "shell.execute_reply.started": "2025-01-02T16:52:38.288929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_data = df.isnull().sum()\n",
    "\n",
    "# Option 1: Remove rows with missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Option 2: Impute missing values\n",
    "# For numerical columns, fill missing values with the mean\n",
    "df_filled = df.copy()  # Create a copy of the original DataFrame\n",
    "\n",
    "# Impute missing numerical values with the mean\n",
    "numerical_columns = df_filled.select_dtypes(include=['float64', 'int64']).columns\n",
    "df_filled[numerical_columns] = df_filled[numerical_columns].fillna(df_filled[numerical_columns].mean())\n",
    "\n",
    "# For categorical columns, fill missing values with the mode (most frequent value)\n",
    "categorical_columns = df_filled.select_dtypes(include=['object']).columns\n",
    "for column in categorical_columns:\n",
    "    df_filled[column] = df_filled[column].fillna(df_filled[column].mode()[0])\n",
    "\n",
    "# Check again for missing values\n",
    "missing_data_after = df_filled.isnull().sum()\n",
    "\n",
    "# Print the results\n",
    "print(\"Missing data before imputation:\\n\", missing_data)\n",
    "print(\"\\nMissing data after imputation:\\n\", missing_data_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Encode Categorical Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T16:52:43.693073Z",
     "iopub.status.busy": "2025-01-02T16:52:43.692641Z",
     "iopub.status.idle": "2025-01-02T16:52:43.729179Z",
     "shell.execute_reply": "2025-01-02T16:52:43.728045Z",
     "shell.execute_reply.started": "2025-01-02T16:52:43.693035Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to categorical columns\n",
    "df['PU_Address_encoded'] = label_encoder.fit_transform(df['PU_Address'])\n",
    "df['Status_encoded'] = label_encoder.fit_transform(df['Status'])\n",
    "\n",
    "# Check the new columns\n",
    "df[['PU_Address', 'PU_Address_encoded', 'Status', 'Status_encoded']].head()\n",
    "\n",
    "# Use pandas get_dummies for one-hot encoding\n",
    "df_encoded = pd.get_dummies(df, columns=['PU_Address', 'Status'], drop_first=True)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Check the result\n",
    "df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scale or Normalize Numerical Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T17:39:45.744830Z",
     "iopub.status.busy": "2025-01-02T17:39:45.744326Z",
     "iopub.status.idle": "2025-01-02T17:39:45.756595Z",
     "shell.execute_reply": "2025-01-02T17:39:45.755590Z",
     "shell.execute_reply.started": "2025-01-02T17:39:45.744795Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert 'Time' column to datetime\n",
    "df['Time'] = pd.to_datetime(df['Time'], errors='coerce')\n",
    "\n",
    "# Extract the hour from the 'Time' column\n",
    "df['hour'] = df['Time'].dt.hour\n",
    "\n",
    "# Verify the new column\n",
    "print(df[['Time', 'hour']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Min-Max Scaling:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T16:45:42.828591Z",
     "iopub.status.busy": "2025-01-02T16:45:42.828217Z",
     "iopub.status.idle": "2025-01-02T16:45:42.858175Z",
     "shell.execute_reply": "2025-01-02T16:45:42.857274Z",
     "shell.execute_reply.started": "2025-01-02T16:45:42.828563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMax scaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Check the available columns in the dataset\n",
    "print(\"Columns in the dataset:\", df.columns)\n",
    "\n",
    "# Assuming 'hour' is a numerical column, scale it\n",
    "df_normalized = df.copy()\n",
    "\n",
    "# Apply MinMax Scaling to the 'hour' column (replace it with any other numerical column)\n",
    "df_normalized[['hour']] = min_max_scaler.fit_transform(df[['hour']])\n",
    "\n",
    "# Check the normalized data\n",
    "df_normalized.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Split the Dataset into Training and Testing Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T16:45:42.859897Z",
     "iopub.status.busy": "2025-01-02T16:45:42.859480Z",
     "iopub.status.idle": "2025-01-02T16:45:42.986036Z",
     "shell.execute_reply": "2025-01-02T16:45:42.985014Z",
     "shell.execute_reply.started": "2025-01-02T16:45:42.859851Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'hour' is the target variable, replace it if necessary\n",
    "X = df_encoded.drop('hour', axis=1)  # Features (independent variables)\n",
    "y = df_encoded['hour']  # Target variable (dependent variable)\n",
    "\n",
    "# Split the data into training and testing sets (e.g., 80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of the splits\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Other Necessary Transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T16:45:42.987401Z",
     "iopub.status.busy": "2025-01-02T16:45:42.987073Z",
     "iopub.status.idle": "2025-01-02T16:45:43.007164Z",
     "shell.execute_reply": "2025-01-02T16:45:43.005758Z",
     "shell.execute_reply.started": "2025-01-02T16:45:42.987374Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats  # Calculate Z-scores\n",
    "\n",
    "# Check the column names to identify the correct one for Z-score calculation\n",
    "print(\"Columns in dataset:\", df.columns)\n",
    "\n",
    "# For example, using 'hour' as the target column (change this as needed)\n",
    "z_scores = stats.zscore(df['a'])\n",
    "\n",
    "# Set a threshold for outliers (e.g., |Z| > 3)\n",
    "df_no_outliers = df[(z_scores < 3) & (z_scores > -3)]\n",
    "\n",
    "# Check the cleaned data\n",
    "df_no_outliers.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Step 1: Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T17:13:59.836571Z",
     "iopub.status.busy": "2025-01-02T17:13:59.836121Z",
     "iopub.status.idle": "2025-01-02T17:13:59.851549Z",
     "shell.execute_reply": "2025-01-02T17:13:59.850639Z",
     "shell.execute_reply.started": "2025-01-02T17:13:59.836539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume your dataframe 'df' is already loaded\n",
    "# Ensure 'Time' column is in the correct format and handle missing or invalid values\n",
    "df['Time'] = pd.to_datetime(df['Time'], errors='coerce')  # Coerce invalid parsing to NaT\n",
    "df = df.dropna(subset=['Time'])  # Drop rows where 'Time' couldn't be parsed\n",
    "df['hour'] = df['Time'].dt.hour  # Extract hour from 'Time'\n",
    "\n",
    "# Ensure the required columns are present\n",
    "if not {'hour', 'PU_Address_encoded', 'Status_encoded'}.issubset(df.columns):\n",
    "    print(\"Error: Required columns are missing in the dataset.\")\n",
    "else:\n",
    "    # Select features and target\n",
    "    X = df[['hour', 'PU_Address_encoded', 'Status_encoded']]\n",
    "    y = df['Status_encoded']\n",
    "\n",
    "    # Standardize the feature data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Initialize and train the model\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(\"Model Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Prepare the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T17:18:05.397598Z",
     "iopub.status.busy": "2025-01-02T17:18:05.397203Z",
     "iopub.status.idle": "2025-01-02T17:18:05.405814Z",
     "shell.execute_reply": "2025-01-02T17:18:05.404427Z",
     "shell.execute_reply.started": "2025-01-02T17:18:05.397567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if 'Time' in df.columns:\n",
    "    df['Time'] = pd.to_datetime(df['Time'], errors='coerce')  # Convert to datetime\n",
    "    df['hour'] = df['Time'].dt.hour  # Extract hour from Time\n",
    "    print(\"Column 'hour' created successfully.\")\n",
    "else:\n",
    "    print(\"Error: The 'Time' column is missing; 'hour' cannot be generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T17:18:20.740051Z",
     "iopub.status.busy": "2025-01-02T17:18:20.739622Z",
     "iopub.status.idle": "2025-01-02T17:18:20.750802Z",
     "shell.execute_reply": "2025-01-02T17:18:20.749890Z",
     "shell.execute_reply.started": "2025-01-02T17:18:20.740019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if 'PU_Address' in df.columns and 'PU_Address_encoded' not in df.columns:\n",
    "    df['PU_Address_encoded'] = df['PU_Address'].astype('category').cat.codes\n",
    "    print(\"Column 'PU_Address_encoded' created successfully.\")\n",
    "\n",
    "if 'DO_Address' in df.columns and 'DO_Address_encoded' not in df.columns:\n",
    "    df['DO_Address_encoded'] = df['DO_Address'].astype('category').cat.codes\n",
    "    print(\"Column 'DO_Address_encoded' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Feature Scaling (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T17:18:33.622058Z",
     "iopub.status.busy": "2025-01-02T17:18:33.621620Z",
     "iopub.status.idle": "2025-01-02T17:18:33.629936Z",
     "shell.execute_reply": "2025-01-02T17:18:33.628542Z",
     "shell.execute_reply.started": "2025-01-02T17:18:33.621990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if 'Status' in df.columns and 'Status_encoded' not in df.columns:\n",
    "    df['Status_encoded'] = df['Status'].astype('category').cat.codes\n",
    "    print(\"Column 'Status_encoded' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T17:18:49.041557Z",
     "iopub.status.busy": "2025-01-02T17:18:49.041222Z",
     "iopub.status.idle": "2025-01-02T17:18:49.051058Z",
     "shell.execute_reply": "2025-01-02T17:18:49.049772Z",
     "shell.execute_reply.started": "2025-01-02T17:18:49.041531Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check and create 'hour'\n",
    "if 'Time' in df.columns:\n",
    "    df['Time'] = pd.to_datetime(df['Time'], errors='coerce')  # Convert to datetime\n",
    "    df['hour'] = df['Time'].dt.hour  # Extract hour\n",
    "elif 'hour' not in df.columns:\n",
    "    import numpy as np\n",
    "    df['hour'] = np.random.randint(0, 24, size=len(df))  # Random values\n",
    "    print(\"Column 'hour' created with random values.\")\n",
    "\n",
    "# Check and create encoded columns for addresses\n",
    "if 'PU_Address' in df.columns and 'PU_Address_encoded' not in df.columns:\n",
    "    df['PU_Address_encoded'] = df['PU_Address'].astype('category').cat.codes\n",
    "if 'PU_Address_encoded' not in df.columns:\n",
    "    df['PU_Address_encoded'] = 0  # Default placeholder\n",
    "\n",
    "if 'DO_Address' in df.columns and 'DO_Address_encoded' not in df.columns:\n",
    "    df['DO_Address_encoded'] = df['DO_Address'].astype('category').cat.codes\n",
    "if 'DO_Address_encoded' not in df.columns:\n",
    "    df['DO_Address_encoded'] = 0  # Default placeholder\n",
    "\n",
    "# Check and create 'Status_encoded'\n",
    "if 'Status' in df.columns and 'Status_encoded' not in df.columns:\n",
    "    df['Status_encoded'] = df['Status'].astype('category').cat.codes\n",
    "if 'Status_encoded' not in df.columns:\n",
    "    df['Status_encoded'] = 1  # Default placeholder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Evaluate the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on the Test Set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T17:19:55.080244Z",
     "iopub.status.busy": "2025-01-02T17:19:55.079826Z",
     "iopub.status.idle": "2025-01-02T17:19:55.091310Z",
     "shell.execute_reply": "2025-01-02T17:19:55.090123Z",
     "shell.execute_reply.started": "2025-01-02T17:19:55.080210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define features and target\n",
    "X = df[['hour', 'PU_Address_encoded', 'DO_Address_encoded']]\n",
    "y = df['Status_encoded']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(\"Data split into training and testing sets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Model:\n",
    "python\n",
    "Copy code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T17:20:08.582617Z",
     "iopub.status.busy": "2025-01-02T17:20:08.582223Z",
     "iopub.status.idle": "2025-01-02T17:20:08.760416Z",
     "shell.execute_reply": "2025-01-02T17:20:08.759422Z",
     "shell.execute_reply.started": "2025-01-02T17:20:08.582585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model trained successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T17:22:59.470274Z",
     "iopub.status.busy": "2025-01-02T17:22:59.469828Z",
     "iopub.status.idle": "2025-01-02T17:22:59.674207Z",
     "shell.execute_reply": "2025-01-02T17:22:59.673126Z",
     "shell.execute_reply.started": "2025-01-02T17:22:59.470245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure dataset is ready (your fixed dataset)\n",
    "# df = ...\n",
    "\n",
    "# Define features and target\n",
    "X = df[['hour', 'PU_Address_encoded', 'DO_Address_encoded']]\n",
    "y = df['Status_encoded']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(\"Data split into training and testing sets.\")\n",
    "\n",
    "# Train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model trained successfully.\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Example new data for prediction\n",
    "import pandas as pd\n",
    "\n",
    "# Example new data for prediction (convert list to DataFrame)\n",
    "new_data = pd.DataFrame([[12, 3, 2]], columns=['hour', 'PU_Address_encoded', 'DO_Address_encoded'])\n",
    "\n",
    "# Predict using the trained model\n",
    "predicted_status = model.predict(new_data)\n",
    "print(f\"Predicted Status for {new_data.values.tolist()[0]}: {predicted_status[0]}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6394689,
     "sourceId": 10327687,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
